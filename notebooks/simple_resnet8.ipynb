{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "\n",
    "from magicpacket.dataset import features\n",
    "from magicpacket.models import simple_audio_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../data/mini_speech_commands\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset and get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(DATASET_PATH)\n",
    "origin = \"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\"  # noqa: E501\n",
    "if not data_dir.exists():\n",
    "    tf.keras.utils.get_file(\n",
    "        \"mini_speech_commands.zip\",\n",
    "        origin=origin,\n",
    "        extract=True,\n",
    "        cache_dir=\".\",\n",
    "        cache_subdir=\"data\",\n",
    "    )\n",
    "\n",
    "labels = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "labels = labels[labels != \"README.md\"]\n",
    "print(\"labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(path, splits):\n",
    "    datasets = []\n",
    "    file_paths = tf.io.gfile.glob(path + \"/*/*\")\n",
    "    file_paths = tf.random.shuffle(file_paths)\n",
    "    n_rows, split_start = len(file_paths), 0\n",
    "    for split in splits:\n",
    "        n_split = int(split * n_rows)\n",
    "        split_end = split_start + n_split\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(file_paths[split_start:split_end])\n",
    "        datasets.append(dataset)\n",
    "        split_start += n_split\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def get_label(file_path):\n",
    "    return tf.strings.split(input=file_path, sep=os.path.sep)[-2]\n",
    "\n",
    "\n",
    "def get_waveform(file_path):\n",
    "    tensor = tf.io.read_file(file_path)\n",
    "    # The decode wave will be normalized to the range [-1, 1]\n",
    "    audio, _ = tf.audio.decode_wav(contents=tensor)\n",
    "    return tf.squeeze(audio, axis=-1)\n",
    "\n",
    "\n",
    "def get_waveform_and_label(file_path):\n",
    "    return get_waveform(file_path), get_label(file_path)\n",
    "\n",
    "\n",
    "def plot_from_ds(ds, rows=3, cols=3):\n",
    "    n = rows * cols\n",
    "    _, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
    "\n",
    "    for i, (audio, label) in enumerate(ds.take(n)):\n",
    "        r = i // cols\n",
    "        c = i % cols\n",
    "        ax = axes[r][c]\n",
    "        ax.plot(audio.numpy())\n",
    "        ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "        label = label.numpy().decode(\"utf-8\")\n",
    "        ax.set_title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset into tf.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = get_datasets(DATASET_PATH, (0.8, 0.1, 0.1))\n",
    "waveform_ds = train_ds.map(\n",
    "    map_func=get_waveform_and_label, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_from_ds(waveform_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for waveform, label in waveform_ds.take(1):\n",
    "    label = label.numpy().decode(\"utf-8\")\n",
    "    waveform = waveform\n",
    "    mfcc = features.mfcc(waveform)\n",
    "\n",
    "print(\"Label:\", label)\n",
    "print(\"Waveform shape:\", waveform.shape)\n",
    "print(\"Audio playback\")\n",
    "display.display(display.Audio(waveform, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax):\n",
    "    if len(spectrogram.shape) > 2:\n",
    "        assert len(spectrogram.shape) == 3\n",
    "        spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "    # Convert the frequencies to log scale and transpose, so that the time is\n",
    "    # represented on the x-axis (columns).\n",
    "    # Add an epsilon to avoid taking a log of zero.\n",
    "    log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "    height = log_spec.shape[0]\n",
    "    width = log_spec.shape[1]\n",
    "    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "    Y = range(height)\n",
    "    ax.pcolormesh(X, Y, log_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(3, figsize=(10, 12))\n",
    "\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title(label)\n",
    "axes[0].set_xlim([0, 16000])\n",
    "\n",
    "spectrogram = features.spectrogram(waveform)\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title(\"spectrogram\")\n",
    "\n",
    "mfcc = features.mfcc(S=spectrogram)\n",
    "height, width = mfcc.shape\n",
    "X = np.linspace(0, np.size(mfcc), num=width, dtype=int)\n",
    "Y = range(height)\n",
    "axes[2].pcolormesh(X, Y, mfcc.numpy())\n",
    "axes[2].set_title(\"mfcc\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_and_label_id(file_path):\n",
    "    waveform, label = get_waveform_and_label(file_path)\n",
    "    label_id = tf.argmax(label == labels)  # labels is global!\n",
    "    # Add a `channels` dimension, so that the spectrogram can be used\n",
    "    # as image-like input data with convolution layers (which expect\n",
    "    # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "    mfcc = features.mfcc(waveform)[..., tf.newaxis]\n",
    "    return mfcc, label_id\n",
    "\n",
    "\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(map_func=get_mfcc_and_label_id, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds = preprocess_dataset(train_ds)\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "test_ds = preprocess_dataset(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mfcc, _ in train_ds.take(1):\n",
    "    input_shape = mfcc.shape\n",
    "print(\"Input shape:\", input_shape)\n",
    "\n",
    "model = simple_audio_model(train_ds, input_shape, len(labels))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.plot(history.epoch, metrics[\"loss\"], metrics[\"val_loss\"])\n",
    "plt.legend([\"loss\", \"val_loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "    test_audio.append(audio.numpy())\n",
    "    test_labels.append(label.numpy())\n",
    "\n",
    "test_audio = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f\"Test set accuracy: {test_acc:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx, xticklabels=labels, yticklabels=labels, annot=True, fmt=\"g\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_file = data_dir/'no/01bb6a2a_nohash_0.wav'\n",
    "\n",
    "# sample_ds = preprocess_dataset(get_datasets([str(sample_file)], (1))\n",
    "\n",
    "# for mfcc, label in sample_ds.batch(1):\n",
    "#     prediction = model(mfcc)\n",
    "#     plt.bar(commands, tf.nn.softmax(prediction[0]))\n",
    "#     plt.title(f'Predictions for \"{commands[label[0]]}\"')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
