{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KatMU8uFo41o"
   },
   "source": [
    "# Few-Shot KWS\n",
    "\n",
    "This notebook heavily borrows from the work done here:\n",
    "- [github.com/harvard-edge/multilingual_kws](https://github.com/harvard-edge/multilingual_kws)\n",
    "- [multilingual_kws_intro_tutorial.ipynb](https://colab.research.google.com/github/harvard-edge/multilingual_kws/blob/main/multilingual_kws_intro_tutorial.ipynb#scrollTo=rK2Bow1THEvp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZG3qhyy5g2fR"
   },
   "outputs": [],
   "source": [
    "%shell apt-get -qq install sox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTDigUj0TGq1"
   },
   "outputs": [],
   "source": [
    "%pip install samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7W6XgKCvNwm"
   },
   "outputs": [],
   "source": [
    "%shell git clone https://github.com/harvard-edge/multilingual_kws/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FZyx-bMmCBS"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/content/multilingual_kws/\")\n",
    "\n",
    "import absl\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile\n",
    "import tensorflow as tf\n",
    "from absl import logging as absl_logging\n",
    "\n",
    "absl_logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from google.colab.output import eval_js\n",
    "from IPython.display import HTML, Audio, display\n",
    "from multilingual_kws.embedding import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNtrxhbcd2qc"
   },
   "outputs": [],
   "source": [
    "KEYWORD = \"tiempo\"\n",
    "SAMPLE_RATE = 16000\n",
    "SAMPLES_DIR = \"/content/samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8LYsAUXlxF_"
   },
   "source": [
    "# Downloads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8XE8pzDlbYT"
   },
   "outputs": [],
   "source": [
    "assets = [\n",
    "    (\n",
    "        \"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\",\n",
    "        \"/content/speech_commands\",\n",
    "    ),\n",
    "    (\n",
    "        \"https://github.com/harvard-edge/multilingual_kws/releases/download/v0.1-alpha/multilingual_context_73_0.8011.tar.gz\",  # noqa\n",
    "        \"/content/embedding_model\",\n",
    "    ),\n",
    "    (\n",
    "        \"https://github.com/harvard-edge/multilingual_kws/releases/download/v0.1-alpha/unknown_files.tar.gz\",  # noqa\n",
    "        \"/content/unknown_files\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "for origin, cache_subdir in assets:\n",
    "    tf.keras.utils.get_file(origin=origin, untar=True, cache_subdir=cache_subdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZdDNJoOv2FC"
   },
   "source": [
    "# Samples\n",
    "\n",
    "Record around ~20 samples of the *KEYWORD* above. The first 3-5 will be used for visualization and training purposes. The rest will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvUN5iq0v48X"
   },
   "outputs": [],
   "source": [
    "SAMPLES_HTML = HTML(\n",
    "    \"\"\"\n",
    "<script>\n",
    "const audioCtx = new (window.AudioContext || window.webkitAudioContext)();\n",
    "const doneBtn = document.getElementById(\"done-btn\");\n",
    "const keepBtn = document.getElementById(\"keep-btn\");\n",
    "const keptLbl = document.getElementById(\"kept-lbl\");\n",
    "const playBtn = document.getElementById(\"play-btn\");\n",
    "const recordBtn = document.getElementById(\"record-btn\");\n",
    "const sampleRate = audioCtx.sampleRate;\n",
    "\n",
    "let done, keep, numSamples = 0, sample;\n",
    "const promise = () =>\n",
    "    new Promise((resolve) => {\n",
    "        done = (isDone = true) => {\n",
    "            doneBtn.disabled = true;\n",
    "            keepBtn.disabled = true;\n",
    "            playBtn.disabled = true;\n",
    "            recordBtn.disabled = true;\n",
    "            if (sample)\n",
    "                keptLbl.innerHTML = ++numSamples;\n",
    "            resolve(isDone);\n",
    "        };\n",
    "        keep = () => done(false);\n",
    "    });\n",
    "\n",
    "const getSample = async () => {\n",
    "    doneBtn.disabled = numSamples === 0;\n",
    "    recordBtn.disabled = false;\n",
    "    const done = await promise();\n",
    "    const result = JSON.stringify({\n",
    "        done,\n",
    "        sampleRate,\n",
    "        sample\n",
    "    });\n",
    "    keepBtn.disabled = true;\n",
    "    playBtn.disabled = true;\n",
    "    recordBtn.disabled = true;\n",
    "    doneBtn.disabled = true;\n",
    "    sample = null;\n",
    "    return result;\n",
    "};\n",
    "\n",
    "const captureAudio = (analyser, duration) => {\n",
    "    const fftSize = analyser.fftSize;\n",
    "    let intervalID, numIntervals = Math.floor(sampleRate * duration / fftSize);\n",
    "    const timeDomainData = new Uint8Array(fftSize);\n",
    "    const timeDomainDataQueue = [];\n",
    "\n",
    "    return new Promise(resolve => {\n",
    "        const getByteTimeDomainData = () => {\n",
    "            analyser.getByteTimeDomainData(timeDomainData);\n",
    "            timeDomainData.forEach(byte => timeDomainDataQueue.push(byte / 128 - 1));\n",
    "            if (--numIntervals === 0) {\n",
    "                clearInterval(intervalID);\n",
    "                resolve(timeDomainDataQueue);\n",
    "            }\n",
    "        };\n",
    "\n",
    "        intervalID = setInterval(getByteTimeDomainData, (fftSize / sampleRate) * 1e3);\n",
    "    });\n",
    "};\n",
    "\n",
    "const play = () => {\n",
    "    const buffer = audioCtx.createBuffer(1, sample.length, sampleRate);\n",
    "    const buffering = buffer.getChannelData(0);\n",
    "    sample.forEach((value, index) => buffering[index] = value);\n",
    "\n",
    "    const source = audioCtx.createBufferSource();\n",
    "    source.buffer = buffer;\n",
    "    source.connect(audioCtx.destination);\n",
    "    source.start(0);\n",
    "};\n",
    "\n",
    "const record = async () => {\n",
    "    keepBtn.disabled = true;\n",
    "    playBtn.disabled = true;\n",
    "    recordBtn.disabled = true;\n",
    "    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "    const source = audioCtx.createMediaStreamSource(stream);\n",
    "    const analyser = source.context.createAnalyser();\n",
    "    analyser.fftSize = 2048;\n",
    "    analyser.smoothingTimeConstant = 0;\n",
    "    source.connect(analyser);\n",
    "\n",
    "    sample = await captureAudio(analyser, 1);\n",
    "\n",
    "    analyser.disconnect();\n",
    "    source.disconnect();\n",
    "    stream.getTracks().forEach((track) => track.stop());\n",
    "    doneBtn.disabled = false;\n",
    "    keepBtn.disabled = false;\n",
    "    playBtn.disabled = false;\n",
    "    recordBtn.disabled = false;\n",
    "};\n",
    "</script>\n",
    "\n",
    "<div>\n",
    "    <button id=\"play-btn\" disabled=true onclick=\"play()\">Play</button>\n",
    "    <button id=\"record-btn\" onclick=\"record()\">Record</button>\n",
    "    <button id=\"keep-btn\" disabled=true onclick=\"keep()\">Keep</button>\n",
    "    <button id=\"done-btn\" disabled=true onclick=\"done()\">Done</button>\n",
    "    <div>\n",
    "        <div style=\"display:inline-block\">\n",
    "            <p>Samples: </p>\n",
    "        </div>\n",
    "        <div style=\"display:inline-block\">\n",
    "            <label id=\"kept-lbl\">0</label>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xo9AWEuZL6G0"
   },
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    shutil.rmtree(SAMPLES_DIR, ignore_errors=True)\n",
    "    os.mkdir(SAMPLES_DIR)\n",
    "    display(SAMPLES_HTML)\n",
    "    count = 0\n",
    "    while True:\n",
    "        result = json.loads(eval_js(\"getSample()\"))\n",
    "        rate_in, rate_out = result[\"sampleRate\"], SAMPLE_RATE\n",
    "        sample = result[\"sample\"]\n",
    "        if sample:\n",
    "            audio = np.array(sample, dtype=np.float32).reshape((len(sample),))\n",
    "            resampled = librosa.resample(\n",
    "                audio,\n",
    "                orig_sr=rate_in,\n",
    "                target_sr=rate_out,\n",
    "                res_type=\"kaiser_fast\",\n",
    "                fix=True,\n",
    "            )\n",
    "            soundfile.write(f\"{SAMPLES_DIR}/{count}.wav\", resampled, rate_out, \"PCM_16\")\n",
    "        if result[\"done\"]:\n",
    "            break\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRs2bM8EFMxf"
   },
   "outputs": [],
   "source": [
    "get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29y-UDDEgEZk"
   },
   "outputs": [],
   "source": [
    "samples = list(sorted(Path(SAMPLES_DIR).glob(\"*.wav\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quwZbo4yX5U9"
   },
   "outputs": [],
   "source": [
    "for sample in samples[:3]:\n",
    "    display(Audio(str(sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BbdzkV0fHHu"
   },
   "outputs": [],
   "source": [
    "settings = input_data.standard_microspeech_model_settings(label_count=1)\n",
    "fig, axes = plt.subplots(ncols=3)\n",
    "for sample, ax in zip(samples[:3], axes):\n",
    "    spectrogram = input_data.file2spec(settings, str(sample))  # PosixPath not supported\n",
    "    ax.imshow(spectrogram.numpy())\n",
    "    ax.set_title(sample.parts[2:])\n",
    "fig.set_size_inches(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN8DkJRog-e9"
   },
   "outputs": [],
   "source": [
    "print(subprocess.check_output([\"soxi\", samples[0]]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXxSFcFtmkz2"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsRd8C_ul_cO"
   },
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "base_model = tf.keras.models.load_model(\n",
    "    \"./embedding_model/multilingual_context_73_0.8011\"\n",
    ")\n",
    "tf.get_logger().setLevel(logging.INFO)\n",
    "\n",
    "embedding = tf.keras.models.Model(\n",
    "    name=\"embedding_model\",\n",
    "    inputs=base_model.inputs,\n",
    "    outputs=base_model.get_layer(name=\"dense_2\").output,\n",
    ")\n",
    "embedding.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We4eDYosmnbb"
   },
   "outputs": [],
   "source": [
    "sample_fpath = str(samples[0])\n",
    "print(\"Filepath:\", sample_fpath)\n",
    "spectrogram = input_data.file2spec(settings, sample_fpath)\n",
    "print(\"Spectrogram shape\", spectrogram.shape)\n",
    "# retrieve embedding vector representation (reshape into 1x49x40x1)\n",
    "feature_vec = embedding.predict(spectrogram[tf.newaxis, :, :, tf.newaxis])\n",
    "print(\"Feature vector shape:\", feature_vec.shape)\n",
    "plt.plot(feature_vec[0])\n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2nLMajoi4_7"
   },
   "outputs": [],
   "source": [
    "CATEGORIES = 3  # silence + unknown + target_keyword\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        embedding,\n",
    "        tf.keras.layers.Dense(units=18, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dense(units=CATEGORIES, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVOwYItwuH3y"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmTJU3NAvZ1d"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfrRqcnruXe8"
   },
   "outputs": [],
   "source": [
    "model_settings = input_data.standard_microspeech_model_settings(3)\n",
    "\n",
    "unknown_files_txt = \"/content/unknown_files/unknown_files.txt\"\n",
    "unknown_files = []\n",
    "with open(unknown_files_txt) as fh:\n",
    "    for w in fh.read().splitlines():\n",
    "        unknown_files.append(\"/content/unknown_files/\" + w)\n",
    "\n",
    "audio_dataset = input_data.AudioDataset(\n",
    "    model_settings=model_settings,\n",
    "    commands=[KEYWORD],\n",
    "    background_data_dir=\"/content/speech_commands/_background_noise_/\",\n",
    "    unknown_files=unknown_files,\n",
    "    unknown_percentage=50.0,\n",
    "    spec_aug_params=input_data.SpecAugParams(percentage=80),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwcNkXcvvVeE"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2L5NDDUvWKL"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 4\n",
    "\n",
    "five_samples = [f\"{SAMPLES_DIR}/{sample.name}\" for sample in samples[:5]]\n",
    "init_train_ds = audio_dataset.init_single_target(\n",
    "    AUTOTUNE, five_samples, is_training=True\n",
    ")\n",
    "train_ds = init_train_ds.shuffle(buffer_size=1000).repeat().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVsaNVLqvxGs"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, steps_per_epoch=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bm0fLCO8wd6u"
   },
   "outputs": [],
   "source": [
    "history = history.history\n",
    "\n",
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0, 2])\n",
    "plt.plot(history[\"loss\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0, 1])\n",
    "plt.plot(history[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nRk3hq6xkHM"
   },
   "outputs": [],
   "source": [
    "model.save(\"fewshotkws.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXuHrEzz1sc8"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdXZdk3qxtKM"
   },
   "outputs": [],
   "source": [
    "test_samples = [f\"{SAMPLES_DIR}/{sample.name}\" for sample in samples[5:]]\n",
    "test_spectrograms = np.array([input_data.file2spec(settings, f) for f in test_samples])\n",
    "# fetch softmax predictions from the finetuned model:\n",
    "# (class 0: silence/background noise, class 1: unknown keyword, class 2: target)\n",
    "predictions = model.predict(test_spectrograms)\n",
    "categorical_predictions = np.argmax(predictions, axis=1)\n",
    "# which predictions match the target class?\n",
    "accuracy = (\n",
    "    categorical_predictions[categorical_predictions == 2].shape[0]\n",
    "    / predictions.shape[0]\n",
    ")\n",
    "print(f\"Test accuracy on testset: {accuracy:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Am-LuWOxyk1W"
   },
   "outputs": [],
   "source": [
    "non_target_examples = []\n",
    "for word in os.listdir(\"speech_commands\"):\n",
    "    if not os.path.isdir(f\"speech_commands/{word}\"):\n",
    "        continue\n",
    "    if word == KEYWORD or word == \"_background_noise_\":\n",
    "        continue\n",
    "    non_target_examples.extend(Path(f\"speech_commands/{word}\").glob(\"*.wav\"))\n",
    "\n",
    "# downsampling list to speed it up\n",
    "rng = np.random.RandomState(42)\n",
    "non_target_examples = rng.choice(non_target_examples, 1000, replace=False).tolist()\n",
    "print(\"Number of non-target examples\", len(non_target_examples))\n",
    "\n",
    "non_target_spectrograms = np.array(\n",
    "    [input_data.file2spec(settings, str(f)) for f in non_target_examples]\n",
    ")\n",
    "# fetch softmax predictions from the finetuned model:\n",
    "# (class 0: silence/background noise, class 1: unknown keyword, class 2: target)\n",
    "predictions = model.predict(non_target_spectrograms)\n",
    "categorical_predictions = np.argmax(predictions, axis=1)\n",
    "# which predictions match the non-target class?\n",
    "accuracy = (\n",
    "    categorical_predictions[categorical_predictions == 1].shape[0]\n",
    "    / predictions.shape[0]\n",
    ")\n",
    "print(f\"Estimated accuracy on non-target samples: {accuracy:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NgtY15yzSZu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qXuHrEzz1sc8"
   ],
   "name": "few-shot-kws.ipynb",
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
