{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vszAGJAaWTB9"
   },
   "source": [
    "This notebook is based on the [Simple audio recognition: Recognizing keywords](https://www.tensorflow.org/tutorials/audio/simple_audio) tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUmmYyRQR7KB"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "IS_COLAB = \"google.colab\" in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "    module_dir = \"./magic-packet\"\n",
    "    if not os.path.exists(module_dir):\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"git\",\n",
    "                \"clone\",\n",
    "                \"-q\",\n",
    "                \"https://github.com/jjgp/magic-packet.git\",\n",
    "                module_dir,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    subprocess.run([\"pip\", \"-q\", \"install\", \"-e\", module_dir], capture_output=True)\n",
    "\n",
    "    content_dir = \"/content/magic-packet/\"\n",
    "    if content_dir not in sys.path:\n",
    "        sys.path.insert(0, \"/content/magic-packet/\")\n",
    "else:\n",
    "    sys.path.insert(0, os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "\n",
    "from magic_packet import datasets, features\n",
    "from magic_packet.models import simple_audio_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1xWc6sDH66w"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_ds, val_ds, test_ds), ds_info = datasets.load(\n",
    "    \"mini_speech_commands\",\n",
    "    split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "names = ds_info.features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_from_ds(ds, names, rows=3, cols=3):\n",
    "    n = rows * cols\n",
    "    _, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
    "\n",
    "    for i, example in enumerate(ds.take(n)):\n",
    "        audio, label = example[\"audio\"], example[\"label\"]\n",
    "        r = i // cols\n",
    "        c = i % cols\n",
    "        ax = axes[r][c]\n",
    "        normalized = features.normalize(audio)\n",
    "        ax.plot(normalized.numpy())\n",
    "        ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "        ax.set_title(names[label])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_from_ds(train_ds, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTMtG5UWIKG7"
   },
   "source": [
    "# Feature extraction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax):\n",
    "    if len(spectrogram.shape) > 2:\n",
    "        assert len(spectrogram.shape) == 3\n",
    "        spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "    # Convert the frequencies to log scale and transpose, so that the time is\n",
    "    # represented on the x-axis (columns).\n",
    "    # Add an epsilon to avoid taking a log of zero.\n",
    "    log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "    height = log_spec.shape[0]\n",
    "    width = log_spec.shape[1]\n",
    "    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "    Y = range(height)\n",
    "    ax.pcolormesh(X, Y, log_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in train_ds.take(1):\n",
    "    audio, label = example[\"audio\"], example[\"label\"]\n",
    "    # the waveform is normalized to the range [-1, 1]\n",
    "    wavename = names[label]\n",
    "    waveform = features.normalize(audio)\n",
    "    mfcc = features.mfcc(waveform)\n",
    "\n",
    "print(\"Name:\", wavename)\n",
    "print(\"Waveform shape:\", waveform.shape)\n",
    "print(\"Audio playback\")\n",
    "display.display(display.Audio(waveform, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(3, figsize=(10, 12))\n",
    "\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title(wavename)\n",
    "axes[0].set_xlim([0, 16000])\n",
    "\n",
    "spectrogram = features.spectrogram(waveform)\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title(\"spectrogram\")\n",
    "\n",
    "mfcc = features.mfcc(S=spectrogram)\n",
    "height, width = mfcc.shape\n",
    "X = np.linspace(0, np.size(mfcc), num=width, dtype=int)\n",
    "Y = range(height)\n",
    "axes[2].pcolormesh(X, Y, mfcc.numpy())\n",
    "axes[2].set_title(\"mfcc\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj1tmbJrlTmJ"
   },
   "source": [
    "# Build and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwqfDMJiwKtH"
   },
   "source": [
    "## Preprocess datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_and_label(example):\n",
    "    audio, label = example[\"audio\"], example[\"label\"]\n",
    "    normalized = features.normalize(audio)\n",
    "    # Add a `channels` dimension, so that the spectrogram can be used\n",
    "    # as image-like input data with convolution layers (which expect\n",
    "    # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "    mfcc = features.mfcc(normalized)[..., tf.newaxis]\n",
    "    return mfcc, label\n",
    "\n",
    "\n",
    "def preprocess_dataset(ds):\n",
    "    return ds.map(map_func=get_mfcc_and_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds = preprocess_dataset(train_ds)\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "test_ds = preprocess_dataset(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSjk514iwITw"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mfcc, _ in train_ds.take(1):\n",
    "    input_shape = mfcc.shape\n",
    "print(\"Input shape:\", input_shape)\n",
    "\n",
    "n_labels = len(names)\n",
    "model = simple_audio_model(train_ds, input_shape, n_labels)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuS9nuvBwHd4"
   },
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.plot(history.epoch, metrics[\"loss\"], metrics[\"val_loss\"])\n",
    "plt.legend([\"loss\", \"val_loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmjRV83GapCm"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "    test_audio.append(audio.numpy())\n",
    "    test_labels.append(label.numpy())\n",
    "\n",
    "test_audio = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f\"Test set accuracy: {test_acc:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx, xticklabels=names, yticklabels=names, annot=True, fmt=\"g\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio, label in test_ds.take(1):\n",
    "    prediction = model(tf.expand_dims(audio, axis=0))\n",
    "    plt.bar(names, tf.nn.softmax(prediction[0]))\n",
    "    plt.title(f'Predictions for \"{names[label]}\"')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
